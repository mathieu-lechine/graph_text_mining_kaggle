{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "training_val_idx = random.sample(range(len(training_set)),math.floor(0.9*len(training_set)))\n",
    "def diff(a, b):\n",
    "        b = set(b)\n",
    "        return [aa for aa in a if aa not in b]\n",
    "testing_val_idx=diff(range(len(training_set)),training_val_idx)\n",
    "\n",
    "train_val = [training_set[i] for i in training_val_idx]\n",
    "test_val = [training_set[i] for i in testing_val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27770\n"
     ]
    }
   ],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "print(len(IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "dates = [element[1] for element in node_info]\n",
    "print(dates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des noms d'auteurs\n",
    "Retrait des parenthèses et des espaces/virgules finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "IDs2 = list()\n",
    "i=0\n",
    "for element in node_info:\n",
    "    drap = 1\n",
    "    temp = re.sub('[!@#<>\\\\/\\'1\\}\\{$]', '', element[3])\n",
    "    temp = re.sub('\\.', ' ', element[3])\n",
    "\n",
    "    \n",
    "\n",
    "    while drap ==1:\n",
    "        idx_parenthese_ouverte = temp.find(\"(\")\n",
    "        idx_parenthese_fermee = temp.find(\")\")\n",
    "        if idx_parenthese_ouverte != -1 and idx_parenthese_fermee != -1:\n",
    "            idx_ouvert2 = temp[(idx_parenthese_ouverte+1):idx_parenthese_fermee].find(\"(\")\n",
    "            if idx_ouvert2 ==-1:\n",
    "                temp = temp[0:(idx_parenthese_ouverte-1)]+temp[(idx_parenthese_fermee+1):]\n",
    "            else:\n",
    "                drap2=1\n",
    "                idx_ouvert_temp = idx_parenthese_ouverte+1\n",
    "                while drap2 ==1:\n",
    "                    idx_ouvert_temp += idx_ouvert2\n",
    "                    idx_ouvert2 = temp[(idx_ouvert_temp+1):idx_parenthese_fermee].find(\"(\")\n",
    "                    if idx_ouvert2==-1:\n",
    "                        temp = temp[0:(idx_ouvert_temp)]+temp[(idx_parenthese_fermee+1):]\n",
    "                        drap2=0\n",
    "        elif idx_parenthese_ouverte != -1:\n",
    "            temp = temp[0:(idx_parenthese_ouverte-1)]\n",
    "        elif idx_parenthese_fermee != -1:\n",
    "            temp = temp[(idx_parenthese_fermee+1):]\n",
    "        elif len(temp) >1 and temp[-1]==' ':\n",
    "            temp=temp[0:-1]\n",
    "        elif len(temp) >1 and temp[-1]==',':\n",
    "            temp=temp[0:-1]\n",
    "        elif len(temp) >1 and temp[-1]=='.':\n",
    "            temp=temp[0:-1]\n",
    "        else :\n",
    "            idx_point = temp.find(\".\")\n",
    "            if idx_point == -1:\n",
    "                drap = 0\n",
    "            elif temp[idx_point+1]!=\" \":\n",
    "                temp=temp[0:idx_point] + \". \"+ temp[(idx_point+1):]\n",
    "            else:\n",
    "                drap = 0\n",
    "\n",
    "    tempsplit = temp.split(\", \")\n",
    "    for j in range(len(tempsplit)-1,0,-1):\n",
    "        if tempsplit[j]=='':\n",
    "            del(tempsplit[j])\n",
    "    i+=1\n",
    "\n",
    "    if np.mod(i,5000)==0:\n",
    "        print(i)\n",
    "    IDs2.append(tempsplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatage des noms\n",
    "On ne garde que l'initiale et le dernier mot (qui fait office de nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur 1 (nom bizarre) :  802 Erreur 2 (pas de nom) :  4033\n"
     ]
    }
   ],
   "source": [
    "res = list()\n",
    "n_erreur=0\n",
    "n_erreur2=0\n",
    "for i in range(len(IDs2)):\n",
    "    drap = 0\n",
    "    temp_list = list()\n",
    "    for j in range(len(IDs2[i])):\n",
    "        # On récupère l'initiale et le dernier mot si c'est possible\n",
    "        # Si impossible on sort des deux boucles et on ne rajoute pas le papier ?\n",
    "        idx_parenthese_ouverte = IDs2[i][j].find(\"(\")\n",
    "        idx_parenthese_fermee = IDs2[i][j].find(\")\")\n",
    "        if idx_parenthese_ouverte != -1 and idx_parenthese_fermee != -1:\n",
    "            print(\"erreur type 1\")\n",
    "            IDs_ij = IDs2[i][j][0:(idx_parenthese_ouverte-1)]+IDs2[i][j][(idx_parenthese_fermee+2):]\n",
    "            print(IDs2[i][j], IDs_ij)\n",
    "        elif idx_parenthese_ouverte != -1:\n",
    "            print(\"erreur type 2\")\n",
    "            IDs_ij = IDs2[i][j][0:(idx_parenthese_ouverte-1)]\n",
    "            print(IDs2[i][j])\n",
    "            print(IDs_ij)\n",
    "        elif idx_parenthese_fermee != -1:\n",
    "            print(\"erreur type 3\")\n",
    "            IDs_ij = IDs2[i][j][(idx_parenthese_fermee+2):]\n",
    "            print(IDs2[i][j])\n",
    "            print(IDs_ij)\n",
    "        else : \n",
    "            IDs_ij = IDs2[i][j]\n",
    "        temp = IDs_ij.split(\" \")\n",
    "        #print(temp)\n",
    "        if temp==[\"\"]:\n",
    "            drap = 2\n",
    "            break\n",
    "        if temp[-1] == \"\" or temp[-1][0].isupper() == False:\n",
    "            drap = 1\n",
    "            break\n",
    "        else:\n",
    "            temp_list.append(temp[0][0].upper() + \". \" + temp[-1])\n",
    "    \n",
    "    if drap == 0:\n",
    "        res.append(temp_list)\n",
    "    elif drap==1:\n",
    "        res.append(None)\n",
    "        n_erreur+=1\n",
    "    else:\n",
    "        res.append(None)\n",
    "        n_erreur2+=1\n",
    "        \n",
    "print(\"Erreur 1 (nom bizarre) : \", n_erreur, \"Erreur 2 (pas de nom) : \", n_erreur2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27770"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stockage du fichier avec les noms corrigés et séparés (potentiellement inutile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo = open('node_info_rectified.csv','w')\n",
    "writer = csv.writer(fo,  lineterminator = '\\n')\n",
    "i=0\n",
    "f = open(\"node_information.csv\", \"r\")\n",
    "reader = csv.reader(f)\n",
    "\n",
    "for row in reader:\n",
    "    row[3] = res[i]\n",
    "    writer.writerow(row)\n",
    "    i+=1\n",
    "\n",
    "fo.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On selectionne les couples d'articles qui sont liés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "couple_article = [ [id1,id2] for id1, id2, edge in train_val if int(edge) == 1 ] \n",
    "list_couple_article_without_edge = [ [id1,id2] for id1, id2, edge in train_val if int(edge) ==0 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301426"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(couple_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un dictionnaire des citations entre auteurs et des coauteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_id_coauthor = {}\n",
    "dict_id_coauthor_strict = {}\n",
    "list_coauthor = list()\n",
    "for i in res:\n",
    "    if i != None:\n",
    "        for j in i:\n",
    "            for k in i:\n",
    "                    dict_id_coauthor[j]=k\n",
    "                    if j != k:\n",
    "                        dict_id_coauthor_strict[j]=k\n",
    "                        list_coauthor.append(list([j, k]))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphe d'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_article = nx.Graph(couple_article)\n",
    "G=graph_article\n",
    "#add isolated nodes\n",
    "G.add_nodes_from(IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de l'attribut \"auteur\" à chaque noeud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(node_info)):\n",
    "    if G.has_node(node_info[i][0]):\n",
    "        G.node[node_info[i][0]]['author'] = res[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Graph_coauthor.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphe d'auteur (arrêtes : cocitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Graph_author = nx.Graph()\n",
    "for node, data in G.nodes_iter(data=True):\n",
    "    if data['author'] != None:\n",
    "        for author in data['author']:\n",
    "            Graph_author.add_node(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M. Cvetic', 'H. Lu', 'C. Pope']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1033"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Graph_author.edge['M. Cvetic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for edge in G.edges_iter():\n",
    "    if G.node[edge[0]]['author'] != None and G.node[edge[1]]['author'] != None:\n",
    "        for author_from in G.node[edge[0]]['author']:\n",
    "            for author_to in G.node[edge[1]]['author']:\n",
    "                Graph_author.add_edge(author_from,author_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Graph_coauthor.edge['A. Kempf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphe de coateurs (arrêtes : corédaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Graph_coauthor = nx.Graph()\n",
    "for node, data in G.nodes_iter(data=True):\n",
    "    if data['author'] != None:\n",
    "        for author in data['author']:\n",
    "            Graph_coauthor.add_node(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for node, data in G.nodes_iter(data=True):\n",
    "    if data['author'] != None:\n",
    "        for author1 in data['author']:\n",
    "            for author2 in data['author']:\n",
    "                if author1 != author2:\n",
    "                    Graph_coauthor.add_edge(author1,author2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de voisins en commun de l'article (normalisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "frac_common_neigh=np.zeros((len(training_set),1))\n",
    "has_common_neigh=np.zeros((len(training_set),1))\n",
    "for i in range(len(training_set)):\n",
    "    if np.mod(i,100000)==0:\n",
    "        print(i)\n",
    "    try:\n",
    "        frac_common_neigh[i,0] = float(len(sorted(nx.common_neighbors(G,training_set[i][0],training_set[i][1]))))/float(len(sorted(nx.neighbors(G,training_set[i][0])))+len(sorted(nx.neighbors(G,training_set[i][1]))))\n",
    "    except:\n",
    "        frac_common_neigh[i,0]=0\n",
    "    has_common_neigh[i,0]=frac_common_neigh[i,0]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de citations en commun des auteurs (normalisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "frac_common_neigh_author=np.zeros((len(training_set),1))\n",
    "has_common_neigh_author=np.zeros((len(training_set),1))\n",
    "for i in range(len(training_set)):\n",
    "    if np.mod(i,100000)==0:\n",
    "        print(i)\n",
    "    frac_common_neigh_author[i,0]=0\n",
    "    if G.node[training_set[i][0]]['author'] != None and G.node[training_set[i][1]]['author'] != None :\n",
    "        sum_temp1 = 0\n",
    "        sum_temp2 = 0\n",
    "        for author1 in G.node[training_set[i][0]]['author']:\n",
    "            for author2 in G.node[training_set[i][1]]['author']:\n",
    "                try:\n",
    "                    frac_common_neigh_author[i,0] += float(len(sorted(nx.common_neighbors(Graph_author,author1,author2))))\n",
    "                    #/float(len(sorted(nx.neighbors(Graph_author,author1)))+len(sorted(nx.neighbors(Graph_author,author2))))\n",
    "                except:\n",
    "                    frac_common_neigh_author[i,0]+=0\n",
    "        for author1 in G.node[training_set[i][0]]['author']:\n",
    "            try:\n",
    "                sum_temp1 += float(len(sorted(nx.neighbors(Graph_author,author1))))\n",
    "            except:\n",
    "                sum_temp1+=0\n",
    "        for author2 in G.node[training_set[i][1]]['author']:\n",
    "            try:\n",
    "                sum_temp2 += float(len(sorted(nx.neighbors(Graph_author,author2))))\n",
    "            except:\n",
    "                sum_temp2+=0\n",
    "        if sum_temp1 !=0 or sum_temp2 !=0:\n",
    "            frac_common_neigh_author[i,0]=100*float(frac_common_neigh_author[i,0])/(sum_temp1+sum_temp2)\n",
    "    has_common_neigh_author[i,0]=frac_common_neigh_author[i,0]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 4.81927711],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 6.06060606],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_common_neigh_coauthor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de citations en commun des coauteurs (normalisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "frac_common_neigh_coauthor=np.zeros((len(training_set),1))\n",
    "has_common_neigh_coauthor=np.zeros((len(training_set),1))\n",
    "for i in range(len(training_set)):\n",
    "    if np.mod(i,100000)==0:\n",
    "        print(i)\n",
    "    frac_common_neigh_coauthor[i,0]=0\n",
    "    if G.node[training_set[i][0]]['author'] != None and G.node[training_set[i][1]]['author'] != None :\n",
    "        sum_temp1 = 0\n",
    "        sum_temp2 = 0\n",
    "        for author1 in G.node[training_set[i][0]]['author']:\n",
    "            for author2 in G.node[training_set[i][1]]['author']:\n",
    "                try:\n",
    "                    frac_common_neigh_coauthor[i,0] += float(len(sorted(nx.common_neighbors(Graph_coauthor,author1,author2))))\n",
    "                except:\n",
    "                    frac_common_neigh_coauthor[i,0]+=0\n",
    "        for author1 in G.node[training_set[i][0]]['author']:\n",
    "            try:\n",
    "                sum_temp1 += float(len(sorted(nx.neighbors(Graph_coauthor,author1))))\n",
    "            except:\n",
    "                sum_temp1+=0\n",
    "        for author2 in G.node[training_set[i][1]]['author']:\n",
    "            try:\n",
    "                sum_temp2 += float(len(sorted(nx.neighbors(Graph_coauthor,author2))))\n",
    "            except:\n",
    "                sum_temp2+=0\n",
    "        if sum_temp1 !=0 or sum_temp2 !=0:\n",
    "            frac_common_neigh_coauthor[i,0]=100*float(frac_common_neigh_coauthor[i,0])/(sum_temp1+sum_temp2)\n",
    "    has_common_neigh_author[i,0]=frac_common_neigh_author[i,0]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553960"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from 'C:\\\\Anaconda\\\\envs\\\\python3\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py'>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mots en commun dans le titre/l'abstract et nombre d'auteurs en commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"titres_nltk.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    titres_nltk  = list(reader)\n",
    "\n",
    "with open(\"abstracts_nltk.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    abstracts_nltk  = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overlap_title=[]\n",
    "overlap_abstract=[]\n",
    "temp_diff=[]\n",
    "comm_auth=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(training_set)):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_title = titres_nltk[index_source]\n",
    "    target_title = titres_nltk[index_target]\n",
    "\n",
    "    source_abstract = abstracts_nltk[index_source]\n",
    "    target_abstract = abstracts_nltk[index_target]\n",
    "\n",
    "    source_auth = res[index_source]\n",
    "    target_auth = res[index_target]\n",
    "\n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    overlap_abstract.append(len(set(source_abstract).intersection(set(target_abstract))))\n",
    "\n",
    "    temp_diff.append(abs(int(dates[index_source]) - int(dates[index_target])))\n",
    "    if source_auth!=None and target_auth != None:\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    else:\n",
    "        comm_auth.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9510123',\n",
       " '1995',\n",
       " 'an infinite number of potentials surrounding 2d black hole',\n",
       " '',\n",
       " 'Phys.Lett.',\n",
       " 'we found an infinite number of potentials surrounding 2d black hole according to the transmission cal t and reflection cal r coefficients for scattering of string fields off 2d black hole we can classify an infinite number of potentials into three graviton-dilaton tachyon and the other types we suggest that the discrete states from all the virasoro levels be candidates for new potentials modes']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de citations de l'article (degré normalisée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp=G.degree()\n",
    "sum_degree = sum(temp[i] for i in G.node)\n",
    "degree_article={}\n",
    "for i in G.node:\n",
    "    degree_article[i]=10**5*temp[i]/float(sum_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de citations des auteurs/coauteurs (degré normalisé et maximum normalisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp=Graph_author.degree()\n",
    "sum_degree = sum(temp[i] for i in Graph_author.node)\n",
    "degree_author={}\n",
    "for i in Graph_author.node:\n",
    "    degree_author[i]=10**5*temp[i]/float(sum_degree)\n",
    "\n",
    "sum_degree_author={}\n",
    "max_degree_author={}   \n",
    "\n",
    "for i in G.nodes_iter():\n",
    "    if G.node[i]['author'] != None:\n",
    "        sum_degree_author[i]=sum(degree_author[author] for author in G.node[i]['author'])\n",
    "        max_degree_author[i]=max(degree_author[author] for author in G.node[i]['author'])\n",
    "    else:\n",
    "        sum_degree_author[i]=0\n",
    "        max_degree_author[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp=Graph_coauthor.degree()\n",
    "sum_degree = sum(temp[i] for i in Graph_coauthor.node)\n",
    "degree_coauthor={}\n",
    "for i in Graph_coauthor.node:\n",
    "    degree_coauthor[i]=10**5*temp[i]/float(sum_degree)\n",
    "\n",
    "sum_degree_coauthor={}\n",
    "max_degree_coauthor={}   \n",
    "\n",
    "for i in G.nodes_iter():\n",
    "    if G.node[i]['author'] != None:\n",
    "        sum_degree_coauthor[i]=sum(degree_coauthor[author] for author in G.node[i]['author'])\n",
    "        max_degree_coauthor[i]=max(degree_coauthor[author] for author in G.node[i]['author'])\n",
    "    else:\n",
    "        sum_degree_coauthor[i]=0\n",
    "        max_degree_coauthor[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base pipeline crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDonnées en commun :\\nNombre de mot dans le titre\\nNombre de mots dans l'abstract\\nNombre d'auteurs en commun\\n\\nNombre de voisins en commun (article)\\nNombre de voisins en commun (auteurs)\\nNombre de voisins en commun (coauteurs)\\n\\nDonnées perso (max, min):\\nDegré (article)\\nSomme degré (auteur)\\nSomme degré (coauteur)\\nMax degré (auteur)\\nMax degré (coauteur)\\n\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Données en commun :\n",
    "Nombre de mot dans le titre\n",
    "Nombre de mots dans l'abstract\n",
    "Ecart temporel (abs)\n",
    "Nombre d'auteurs en commun\n",
    "Has common neigb (article)\n",
    "Has common neigb (auteurs)\n",
    "Has common neigb (coauteurs)\n",
    "Nombre de voisins en commun (article)\n",
    "Somme Nombre de voisins en commun (auteurs)\n",
    "Max Nombre de voisins en commun (auteurs) galère\n",
    "Somme Nombre de voisins en commun (coauteurs)\n",
    "Max Nombre de voisins en commun (coauteurs) galère\n",
    "\n",
    "Données perso (max, min):\n",
    "Between Centralité (article)\n",
    "Between Centralité(auteur)\n",
    "Between Centralité(coauteur)\n",
    "Degré (article)\n",
    "Somme degré (auteur)\n",
    "Somme degré (coauteur)\n",
    "Max degré (auteur)\n",
    "Max degré (coauteur)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec auteurs/coauteurs (tout le monde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_auteur = np.zeros((len(train_val),20))\n",
    "test_auteur = np.zeros((len(test_val),20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553960"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(training_val_idx)):\n",
    "    idx=train_val[i]\n",
    "    j=training_val_idx[i]\n",
    "    train_auteur[i,0]=overlap_title[j]\n",
    "    train_auteur[i,1]=overlap_abstract[j]\n",
    "    train_auteur[i,2]=temp_diff[j]\n",
    "    train_auteur[i,3]=comm_auth[j]\n",
    "    train_auteur[i,4]=has_common_neigh[j]\n",
    "    train_auteur[i,5]=has_common_neigh_author[j]\n",
    "    train_auteur[i,6]=has_common_neigh_coauthor[j]\n",
    "    train_auteur[i,7]=frac_common_neigh[j]\n",
    "    train_auteur[i,8]=frac_common_neigh_author[j]\n",
    "    train_auteur[i,9]=frac_common_neigh_coauthor[j]\n",
    "    train_auteur[i,10]=max(degree_article[idx[0]],degree_article[idx[1]])\n",
    "    train_auteur[i,11]=min(degree_article[idx[0]],degree_article[idx[1]])\n",
    "    train_auteur[i,12]=max(sum_degree_author[idx[0]],sum_degree_author[idx[1]])\n",
    "    train_auteur[i,13]=min(sum_degree_author[idx[0]],sum_degree_author[idx[1]])\n",
    "    train_auteur[i,14]=max(max_degree_author[idx[0]],max_degree_author[idx[1]])\n",
    "    train_auteur[i,15]=min(max_degree_author[idx[0]],max_degree_author[idx[1]])\n",
    "    train_auteur[i,16]=max(sum_degree_coauthor[idx[0]],sum_degree_coauthor[idx[1]])\n",
    "    train_auteur[i,17]=min(sum_degree_coauthor[idx[0]],sum_degree_coauthor[idx[1]])\n",
    "    train_auteur[i,18]=max(max_degree_coauthor[idx[0]],max_degree_coauthor[idx[1]])\n",
    "    train_auteur[i,19]=min(max_degree_coauthor[idx[0]],max_degree_coauthor[idx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(testing_val_idx)):\n",
    "    idx=test_val[i]\n",
    "    j=testing_val_idx[i]\n",
    "    test_auteur[i,0]=overlap_title[j]\n",
    "    test_auteur[i,1]=overlap_abstract[j]\n",
    "    test_auteur[i,2]=temp_diff[j]\n",
    "    test_auteur[i,3]=comm_auth[j]\n",
    "    test_auteur[i,4]=has_common_neigh[j]\n",
    "    test_auteur[i,5]=has_common_neigh_author[j]\n",
    "    test_auteur[i,6]=has_common_neigh_coauthor[j]\n",
    "    test_auteur[i,7]=frac_common_neigh[j]\n",
    "    test_auteur[i,8]=frac_common_neigh_author[j]\n",
    "    test_auteur[i,9]=frac_common_neigh_coauthor[j]\n",
    "    test_auteur[i,10]=max(degree_article[idx[0]],degree_article[idx[1]])\n",
    "    test_auteur[i,11]=min(degree_article[idx[0]],degree_article[idx[1]])\n",
    "    test_auteur[i,12]=max(sum_degree_author[idx[0]],sum_degree_author[idx[1]])\n",
    "    test_auteur[i,13]=min(sum_degree_author[idx[0]],sum_degree_author[idx[1]])\n",
    "    test_auteur[i,14]=max(max_degree_author[idx[0]],max_degree_author[idx[1]])\n",
    "    test_auteur[i,15]=min(max_degree_author[idx[0]],max_degree_author[idx[1]])\n",
    "    test_auteur[i,16]=max(sum_degree_coauthor[idx[0]],sum_degree_coauthor[idx[1]])\n",
    "    test_auteur[i,17]=min(sum_degree_coauthor[idx[0]],sum_degree_coauthor[idx[1]])\n",
    "    test_auteur[i,18]=max(max_degree_coauthor[idx[0]],max_degree_coauthor[idx[1]])\n",
    "    test_auteur[i,19]=min(max_degree_coauthor[idx[0]],max_degree_coauthor[idx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_val = [int(training_set[i][2]) for i in training_val_idx]\n",
    "y_test_val = [int(training_set[i][2]) for i in testing_val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "C:\\Anaconda\\envs\\python3\\lib\\site-packages\\sklearn\\base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=5, min_samples_split=6,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=2,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=5, criterion='gini', max_depth=10, min_samples_split=6, min_samples_leaf=5, max_features='auto', bootstrap=True, n_jobs=2)\n",
    "clf.fit(train_auteur, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_auteur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = precision_recall_fscore_support(y_test_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.94121567,  0.96934112]),\n",
       " array([ 0.96362396,  0.95027296]),\n",
       " array([ 0.95228801,  0.95971234]),\n",
       " array([27848, 33704], dtype=int64))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec auteurs/coauteurs (restreints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2088', '9503001', '1']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDs.index('2008')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans auteurs/coauteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91013394280a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maa\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0maa\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maa\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtesting_val_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_val_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
